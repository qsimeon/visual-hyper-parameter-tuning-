{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0-mlf0gjju-qnmbtb",
      "metadata": {},
      "source": [
        "# Visual Hyperparameter Tuning with Interactive Sliders\n",
        "\n",
        "This notebook demonstrates interactive neural network weight tuning using sliders. You can adjust individual weights in a neural network and immediately see how they affect the model's predictions and decision boundaries.\n",
        "\n",
        "This approach is inspired by sparse auto-encoding and mechanistic interpretability research, allowing us to understand how individual parameters contribute to network behavior.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- Interactive sliders for every weight in the neural network\n",
        "- Real-time visualization of decision boundaries\n",
        "- Activation statistics and weight heatmaps\n",
        "- Multiple dataset options (XOR, circles, moons)\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-1-mlf0gjju-qcrtf4",
      "metadata": {},
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Import from our custom modules\n",
        "from nn_slider_core import SimpleMLPNetwork, create_demo_dataset, compute_activation_statistics\n",
        "from sliders_interface import NetworkSliderInterface, CompactSliderInterface, create_weight_heatmap\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-2-mlf0gjju-8aej15",
      "metadata": {},
      "source": [
        "## Step 1: Create a Demo Dataset\n",
        "\n",
        "We'll start with the classic XOR problem - a non-linearly separable dataset that requires at least one hidden layer to solve.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-3-mlf0gjju-my0cmn",
      "metadata": {},
      "source": [
        "# Create XOR dataset\n",
        "X_train, y_train = create_demo_dataset(task='xor', n_samples=200, noise=0.1)\n",
        "\n",
        "# Visualize the dataset\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='RdYlBu', s=50, alpha=0.7, edgecolors='k')\n",
        "plt.colorbar(scatter, label='Class')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('XOR Dataset')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-4-mlf0gjju-ysyye0",
      "metadata": {},
      "source": [
        "## Step 2: Initialize Neural Network\n",
        "\n",
        "We'll create a simple MLP with one hidden layer. The network architecture is: 2 inputs → 4 hidden units → 1 output.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-5-mlf0gjju-ap51sf",
      "metadata": {},
      "source": [
        "# Create a simple neural network\n",
        "network = SimpleMLPNetwork(input_size=2, hidden_sizes=[4], output_size=1)\n",
        "\n",
        "# Display network information\n",
        "param_info = network.get_parameter_info()\n",
        "print(f\"Total parameters: {network.get_num_parameters()}\")\n",
        "print(\"\\nParameter breakdown:\")\n",
        "for info in param_info:\n",
        "    print(f\"  {info['name']}: shape {info['shape']}, {info['size']} parameters\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-6-mlf0gjju-go1y2q",
      "metadata": {},
      "source": [
        "## Step 3: Visualize Initial Weights\n",
        "\n",
        "Let's visualize the initial random weights as a heatmap to understand the starting configuration.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-7-mlf0gjju-h1fzca",
      "metadata": {},
      "source": [
        "# Create weight heatmap for the first layer\n",
        "fig = create_weight_heatmap(network, layer_idx=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-8-mlf0gjju-zc909x",
      "metadata": {},
      "source": [
        "## Step 4: Helper Function for Decision Boundary Visualization\n",
        "\n",
        "This function creates a mesh grid and visualizes how the network classifies different regions of the input space.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-9-mlf0gjjv-hs33ff",
      "metadata": {},
      "source": [
        "def plot_decision_boundary(network, X, y, resolution=0.02):\n",
        "    \"\"\"Plot the decision boundary of the neural network.\"\"\"\n",
        "    # Create mesh grid\n",
        "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, resolution),\n",
        "                         np.arange(y_min, y_max, resolution))\n",
        "    \n",
        "    # Predict on mesh grid\n",
        "    Z = network.forward(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.contourf(xx, yy, Z, alpha=0.4, cmap='RdYlBu', levels=20)\n",
        "    plt.colorbar(label='Network Output')\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', s=50, alpha=0.8, edgecolors='k', linewidth=1.5)\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title('Decision Boundary Visualization')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Compute and display loss\n",
        "    loss = network.compute_loss(X, y)\n",
        "    plt.text(0.02, 0.98, f'Loss: {loss:.4f}', transform=plt.gca().transAxes,\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n",
        "             verticalalignment='top', fontsize=12)\n",
        "    \n",
        "    plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-10-mlf0gjjv-izwrhq",
      "metadata": {},
      "source": [
        "## Step 5: Initial Decision Boundary (Random Weights)\n",
        "\n",
        "Let's see how the network performs with random initialization before any tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-11-mlf0gjjv-lt79tu",
      "metadata": {},
      "source": [
        "# Plot initial decision boundary\n",
        "plot_decision_boundary(network, X_train, y_train)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-12-mlf0gjjv-oamh79",
      "metadata": {},
      "source": [
        "## Step 6: Interactive Weight Tuning with Sliders\n",
        "\n",
        "Now comes the exciting part! Use the sliders below to adjust individual weights and biases. Watch how each parameter affects the decision boundary and network output.\n",
        "\n",
        "Tips for exploration:\n",
        "\n",
        "- Try adjusting weights in the first layer to see how they affect feature detection\n",
        "- Modify biases to shift decision boundaries\n",
        "- Observe how hidden layer activations change with different weight configurations\n",
        "- Try to manually solve the XOR problem by tuning weights!\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-13-mlf0gjjv-xvl6gn",
      "metadata": {},
      "source": [
        "# Create interactive slider interface\n",
        "slider_interface = NetworkSliderInterface(network, X_train, y_train)\n",
        "slider_interface.display()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-14-mlf0gjjv-l19ivf",
      "metadata": {},
      "source": [
        "## Step 7: Compact Slider Interface (Alternative)\n",
        "\n",
        "For larger networks, we can use a more compact interface that groups parameters by layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-15-mlf0gjjv-uv7x1u",
      "metadata": {},
      "source": [
        "# Create compact slider interface\n",
        "compact_interface = CompactSliderInterface(network, X_train, y_train)\n",
        "compact_interface.display()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-16-mlf0gjjv-d5or0m",
      "metadata": {},
      "source": [
        "## Step 8: Analyze Activation Statistics\n",
        "\n",
        "Let's examine the activation patterns in the hidden layer to understand what features the network has learned.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-17-mlf0gjjv-4q9o7t",
      "metadata": {},
      "source": [
        "# Get current network parameters after tuning\n",
        "current_params = slider_interface.get_current_parameters()\n",
        "network.unvectorize_parameters(current_params)\n",
        "\n",
        "# Forward pass to get activations\n",
        "activations = []\n",
        "x = X_train\n",
        "for i, (W, b) in enumerate(zip(network.weights, network.biases)):\n",
        "    x = np.dot(x, W) + b\n",
        "    if i < len(network.weights) - 1:  # Apply activation for hidden layers\n",
        "        x = np.maximum(0, x)  # ReLU\n",
        "    activations.append(x)\n",
        "\n",
        "# Compute statistics\n",
        "stats = compute_activation_statistics(activations)\n",
        "\n",
        "# Visualize activation statistics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "layers = list(range(len(stats['mean'])))\n",
        "\n",
        "axes[0].plot(layers, stats['mean'], marker='o', linewidth=2, markersize=8)\n",
        "axes[0].set_xlabel('Layer')\n",
        "axes[0].set_ylabel('Mean Activation')\n",
        "axes[0].set_title('Mean Activation by Layer')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(layers, stats['std'], marker='s', linewidth=2, markersize=8, color='orange')\n",
        "axes[1].set_xlabel('Layer')\n",
        "axes[1].set_ylabel('Std Deviation')\n",
        "axes[1].set_title('Activation Std Dev by Layer')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(layers, stats['sparsity'], marker='^', linewidth=2, markersize=8, color='green')\n",
        "axes[2].set_xlabel('Layer')\n",
        "axes[2].set_ylabel('Sparsity (% zeros)')\n",
        "axes[2].set_title('Activation Sparsity by Layer')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-18-mlf0gjjv-m1kbr2",
      "metadata": {},
      "source": [
        "## Step 9: Experiment with Different Datasets\n",
        "\n",
        "Try the same interactive tuning approach with different datasets to see how network behavior changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "cell-19-mlf0gjjv-64t4a3",
      "metadata": {},
      "source": [
        "# Create circles dataset\n",
        "X_circles, y_circles = create_demo_dataset(task='circles', n_samples=200, noise=0.05)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='RdYlBu', s=50, alpha=0.7, edgecolors='k')\n",
        "plt.colorbar(scatter, label='Class')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Circles Dataset')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Create new network for circles\n",
        "network_circles = SimpleMLPNetwork(input_size=2, hidden_sizes=[6], output_size=1)\n",
        "\n",
        "# Interactive interface for circles\n",
        "slider_circles = NetworkSliderInterface(network_circles, X_circles, y_circles)\n",
        "slider_circles.display()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "cell-20-mlf0gjjv-oi9e1m",
      "metadata": {},
      "source": [
        "## Conclusion and Key Insights\n",
        "\n",
        "This notebook demonstrated interactive hyperparameter tuning for neural networks using sliders. Key takeaways:\n",
        "\n",
        "1. Individual weights have varying impacts on the decision boundary - some are more influential than others\n",
        "2. Hidden layer weights determine feature detection, while output weights combine these features\n",
        "3. Biases shift decision boundaries and activation thresholds\n",
        "4. Activation sparsity can indicate which neurons are actively contributing to predictions\n",
        "5. Manual weight tuning provides intuition for how gradient descent optimizes networks\n",
        "\n",
        "This approach connects to sparse auto-encoding and mechanistic interpretability research by allowing us to understand the role of individual parameters. For larger networks, techniques like sparse autoencoders can help identify monosemantic features - individual neurons or directions in activation space that correspond to interpretable concepts.\n",
        "\n",
        "### Further Exploration\n",
        "\n",
        "- Try deeper networks with multiple hidden layers\n",
        "- Experiment with different activation functions\n",
        "- Compare manual tuning vs gradient descent optimization\n",
        "- Investigate weight pruning and sparsity constraints\n",
        "- Apply to real-world datasets and observe interpretability challenges\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}